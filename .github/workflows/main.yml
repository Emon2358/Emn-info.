name: Wayback Machine Scraper

on:
  workflow_dispatch:      # 手動実行のみ

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 waybackpy chardet

    - name: Run scraper
      run: |
        cat << 'EOF' > scraper.py
        import os
        import json
        import chardet
        from datetime import datetime
        from waybackpy import WaybackMachineCDXServerAPI
        import requests
        from bs4 import BeautifulSoup
        import logging

        # ロギング設定
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        URL = "http://www1.rocketbbs.com/310/tabemono.html"
        
        def detect_encoding(content):
            """
            コンテンツのエンコーディングを検出する
            """
            result = chardet.detect(content)
            logger.info(f"Detected encoding: {result}")
            return result['encoding']

        def try_decode_content(content, encodings=['shift_jis', 'euc_jp', 'utf-8', 'cp932']):
            """
            複数のエンコーディングで解読を試みる
            """
            for encoding in encodings:
                try:
                    return content.decode(encoding)
                except UnicodeDecodeError:
                    continue
            
            # 自動検出を試みる
            detected_encoding = detect_encoding(content)
            if detected_encoding:
                try:
                    return content.decode(detected_encoding)
                except UnicodeDecodeError:
                    pass
            
            # 全て失敗した場合はエラーを記録してバイナリを返す
            logger.error("Failed to decode content with all encodings")
            return content.decode('utf-8', errors='replace')

        def get_snapshots():
            user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            cdx = WaybackMachineCDXServerAPI(URL, user_agent)
            snapshots = cdx.snapshots()
            return sorted(snapshots, key=lambda x: x.timestamp)
        
        def fetch_content(snapshot):
            archive_url = f"https://web.archive.org/web/{snapshot.timestamp}/{URL}"
            logger.info(f"Fetching content from: {archive_url}")
            
            response = requests.get(archive_url)
            content = response.content  # バイナリとして取得
            
            # HTMLからエンコーディングを検出
            soup = BeautifulSoup(content, 'html.parser', from_encoding='utf-8')
            meta_encoding = None
            meta_tag = soup.find('meta', charset=True)
            if meta_tag:
                meta_encoding = meta_tag.get('charset')
            elif soup.find('meta', {'http-equiv': 'Content-Type'}):
                content_type = soup.find('meta', {'http-equiv': 'Content-Type'})['content']
                if 'charset' in content_type:
                    meta_encoding = content_type.split('charset=')[-1]
            
            logger.info(f"Meta tag encoding: {meta_encoding}")
            
            # コンテンツの解読を試みる
            if meta_encoding:
                try:
                    return content.decode(meta_encoding)
                except UnicodeDecodeError:
                    pass
            
            return try_decode_content(content)
        
        def clean_text(text):
            """
            テキストのクリーニング処理
            """
            # 不要な空白行の削除
            lines = [line.strip() for line in text.split('\n')]
            lines = [line for line in lines if line]
            return '\n'.join(lines)
        
        def create_readme(snapshots_content):
            output_dir = "tbmn bbs archive"
            os.makedirs(output_dir, exist_ok=True)
            
            with open(f"{output_dir}/README.md", "w", encoding="utf-8") as f:
                f.write("# Tabemono BBS Archive\n\n")
                for timestamp, content in snapshots_content:
                    date = datetime.strptime(timestamp, "%Y%m%d%H%M%S").strftime("%Y-%m-%d %H:%M:%S")
                    f.write(f"## Snapshot: {date}\n\n")
                    
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # テキストの抽出と整形
                    text = clean_text(soup.get_text())
                    f.write(f"```\n{text}\n```\n\n")
                    
                    # デバッグ情報の追加
                    f.write(f"Archive URL: https://web.archive.org/web/{timestamp}/{URL}\n\n")
                    f.write("---\n\n")
        
        def main():
            logger.info("Starting scraping process...")
            snapshots = get_snapshots()
            logger.info(f"Found {len(snapshots)} snapshots")
            
            snapshots_content = []
            
            for snapshot in snapshots:
                try:
                    content = fetch_content(snapshot)
                    snapshots_content.append((snapshot.timestamp, content))
                except Exception as e:
                    logger.error(f"Error processing snapshot {snapshot.timestamp}: {str(e)}")
            
            create_readme(snapshots_content)
            logger.info("Scraping process completed")
        
        if __name__ == "__main__":
            main()
        EOF
        python scraper.py
    
    - name: Commit and push if changed
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add "tbmn bbs archive"
        git commit -m "Update BBS archive with improved encoding $(date +'%Y-%m-%d')" || exit 0
        git push
