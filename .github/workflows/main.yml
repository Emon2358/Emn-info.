name: Wayback Machine BBS Scraper

on:
  workflow_dispatch:
    inputs:
      bbs_url:
        description: 'スクレイピングする掲示板のURL'
        required: true
        type: string
      folder_name:
        description: '保存先フォルダ名'
        required: true
        type: string
      readme_name:
        description: 'READMEファイル名（.mdは自動で付加）'
        required: true
        type: string
      domain_name:
        description: '掲示板のドメイン名（タイトルに使用）'
        required: true
        type: string

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 waybackpy chardet

    - name: Run scraper
      run: |
        cat << 'EOF' > scraper.py
        import os
        import json
        import chardet
        import sys
        from datetime import datetime
        from waybackpy import WaybackMachineCDXServerAPI
        import requests
        from bs4 import BeautifulSoup
        import logging
        import argparse

        # ロギング設定
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        def parse_arguments():
            parser = argparse.ArgumentParser()
            parser.add_argument('--url', required=True, help='Target BBS URL')
            parser.add_argument('--folder', required=True, help='Output folder name')
            parser.add_argument('--readme', required=True, help='README file name')
            parser.add_argument('--domain', required=True, help='Domain name for title')
            return parser.parse_args()
        
        def detect_encoding(content):
            """
            コンテンツのエンコーディングを検出する
            """
            result = chardet.detect(content)
            logger.info(f"Detected encoding: {result}")
            return result['encoding']

        def try_decode_content(content, encodings=['shift_jis', 'euc_jp', 'utf-8', 'cp932']):
            """
            複数のエンコーディングで解読を試みる
            """
            for encoding in encodings:
                try:
                    return content.decode(encoding)
                except UnicodeDecodeError:
                    continue
            
            # 自動検出を試みる
            detected_encoding = detect_encoding(content)
            if detected_encoding:
                try:
                    return content.decode(detected_encoding)
                except UnicodeDecodeError:
                    pass
            
            # 全て失敗した場合はエラーを記録してバイナリを返す
            logger.error("Failed to decode content with all encodings")
            return content.decode('utf-8', errors='replace')

        def get_snapshots(url):
            user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            cdx = WaybackMachineCDXServerAPI(url, user_agent)
            snapshots = cdx.snapshots()
            return sorted(snapshots, key=lambda x: x.timestamp)
        
        def fetch_content(snapshot, url):
            archive_url = f"https://web.archive.org/web/{snapshot.timestamp}/{url}"
            logger.info(f"Fetching content from: {archive_url}")
            
            response = requests.get(archive_url)
            content = response.content  # バイナリとして取得
            
            # HTMLからエンコーディングを検出
            soup = BeautifulSoup(content, 'html.parser', from_encoding='utf-8')
            meta_encoding = None
            meta_tag = soup.find('meta', charset=True)
            if meta_tag:
                meta_encoding = meta_tag.get('charset')
            elif soup.find('meta', {'http-equiv': 'Content-Type'}):
                content_type = soup.find('meta', {'http-equiv': 'Content-Type'})['content']
                if 'charset' in content_type:
                    meta_encoding = content_type.split('charset=')[-1]
            
            logger.info(f"Meta tag encoding: {meta_encoding}")
            
            # コンテンツの解読を試みる
            if meta_encoding:
                try:
                    return content.decode(meta_encoding)
                except UnicodeDecodeError:
                    pass
            
            return try_decode_content(content)
        
        def clean_text(text):
            """
            テキストのクリーニング処理
            """
            # 不要な空白行の削除
            lines = [line.strip() for line in text.split('\n')]
            lines = [line for line in lines if line]
            return '\n'.join(lines)
        
        def create_readme(snapshots_content, args):
            output_dir = args.folder
            os.makedirs(output_dir, exist_ok=True)
            
            readme_path = f"{output_dir}/{args.readme}.md"
            with open(readme_path, "w", encoding="utf-8") as f:
                f.write(f"# {args.domain} BBS Archive\n\n")
                f.write(f"Original URL: {args.url}\n\n")
                f.write(f"Archive created: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\n\n")
                
                for timestamp, content in snapshots_content:
                    date = datetime.strptime(timestamp, "%Y%m%d%H%M%S").strftime("%Y-%m-%d %H:%M:%S")
                    f.write(f"## Snapshot: {date}\n\n")
                    
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # テキストの抽出と整形
                    text = clean_text(soup.get_text())
                    f.write(f"```\n{text}\n```\n\n")
                    
                    # アーカイブURLの追加
                    f.write(f"Archive URL: https://web.archive.org/web/{timestamp}/{args.url}\n\n")
                    f.write("---\n\n")
        
        def main():
            args = parse_arguments()
            logger.info(f"Starting scraping process for URL: {args.url}")
            logger.info(f"Output folder: {args.folder}")
            logger.info(f"README name: {args.readme}")
            
            snapshots = get_snapshots(args.url)
            logger.info(f"Found {len(snapshots)} snapshots")
            
            snapshots_content = []
            
            for snapshot in snapshots:
                try:
                    content = fetch_content(snapshot, args.url)
                    snapshots_content.append((snapshot.timestamp, content))
                except Exception as e:
                    logger.error(f"Error processing snapshot {snapshot.timestamp}: {str(e)}")
            
            create_readme(snapshots_content, args)
            logger.info("Scraping process completed")
        
        if __name__ == "__main__":
            main()
        EOF
        
        # GitHub Actionsの入力パラメータを使用してスクリプトを実行
        python scraper.py \
          --url "${{ github.event.inputs.bbs_url }}" \
          --folder "${{ github.event.inputs.folder_name }}" \
          --readme "${{ github.event.inputs.readme_name }}" \
          --domain "${{ github.event.inputs.domain_name }}"
    
    - name: Commit and push if changed
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add "${{ github.event.inputs.folder_name }}"
        git commit -m "Update BBS archive for ${{ github.event.inputs.domain_name }} ($(date +'%Y-%m-%d'))" || exit 0
        git push
